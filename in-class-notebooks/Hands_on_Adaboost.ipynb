{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On AdaBoost \n",
    "***\n",
    "\n",
    "In this notebook we'll implement the AdaBoost learning algorithm using Decision Stumps as the weak learner.  We'll then use our code on a Handwritten Digit Recognition task.  Finally, we'll do some exploration to better understand the way that AdaBoost is a margin-maximizing learning algorithm. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T17:43:18.639847Z",
     "start_time": "2018-04-30T17:43:17.681317Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import clone \n",
    "import matplotlib.pylab as plt \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Threes and Eights \n",
    "***\n",
    "\n",
    "In this notebook we'll use the downsampled version of MNIST that you used in Homework 4.  Since the flavor of AdaBoost that we'll implement here is inherently binary, we'll subset the dataset into images of $3$'s and $8$'s.  The following class will load and subset the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T17:43:20.447398Z",
     "start_time": "2018-04-30T17:43:20.393032Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ThreesAndEights:\n",
    "    \"\"\"\n",
    "    Class to store MNIST 3s and 8s data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, location):\n",
    "\n",
    "        import pickle, gzip\n",
    "\n",
    "        # Load the dataset\n",
    "        f = gzip.open(location, 'rb')\n",
    "\n",
    "        # Split the data set \n",
    "        X_train, y_train, X_valid, y_valid = pickle.load(f)\n",
    "\n",
    "        # Extract only 3's and 8's for training set \n",
    "        self.X_train = X_train[np.logical_or( y_train==3, y_train == 8), :]\n",
    "        self.y_train = y_train[np.logical_or( y_train==3, y_train == 8)]\n",
    "        self.y_train = np.array([1 if y == 8 else -1 for y in self.y_train])\n",
    "        \n",
    "        # Shuffle the training data \n",
    "        shuff = np.arange(self.X_train.shape[0])\n",
    "        np.random.shuffle(shuff)\n",
    "        self.X_train = self.X_train[shuff,:]\n",
    "        self.y_train = self.y_train[shuff]\n",
    "\n",
    "        # Extract only 3's and 8's for validation set \n",
    "        self.X_valid = X_valid[np.logical_or( y_valid==3, y_valid == 8), :]\n",
    "        self.y_valid = y_valid[np.logical_or( y_valid==3, y_valid == 8)]\n",
    "        self.y_valid = np.array([1 if y == 8 else -1 for y in self.y_valid])\n",
    "        \n",
    "        f.close()\n",
    "        \n",
    "def view_digit(ex, label=None, feature=None):\n",
    "    \"\"\"\n",
    "    function to plot digit examples \n",
    "    \"\"\"\n",
    "    if label: print(\"true label: {:d}\".format(label))\n",
    "    img = ex.reshape(21,21)\n",
    "    col = np.dstack((img, img, img))\n",
    "    if feature is not None: col[feature[0]//21,feature[0]%21,:] = [1, 0, 0]\n",
    "    plt.imshow(col)\n",
    "    plt.xticks([]), plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to load the data.  Then use the `view_digit` function to plot a few examples.  Which numerical labels are assigned to which class?  Does the choice of labels make sense if we plan to implement AdaBoost? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T17:43:21.861048Z",
     "start_time": "2018-04-30T17:43:21.692097Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ThreesAndEights(\"data/mnist21x21_3789.pklz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T17:43:22.650458Z",
     "start_time": "2018-04-30T17:43:22.536920Z"
    }
   },
   "outputs": [],
   "source": [
    "ind = 3\n",
    "view_digit(data.X_train[ind,:], label=data.y_train[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Decision Stumps for MNIST \n",
    "***\n",
    "\n",
    "So what exactly happens if we use a Decision Stump to classify handwritten digits?  Recall that a Decision Stump is simply a Decision Tree with a split on a single feature.  Experiment with the [DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) from sklearn with the `max_depth` parameter set to `1`.  How well does it perform?  Can you determine what feature is being split on?  How can we interpret this in terms of images of handwritten digits? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T17:43:29.361289Z",
     "start_time": "2018-04-30T17:43:29.321771Z"
    }
   },
   "outputs": [],
   "source": [
    "h = DecisionTreeClassifier(max_depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Implementing AdaBoost  \n",
    "***\n",
    "\n",
    "We've given you a skeleton of the class `AdaBoost` below which will train a classifier based on boosted Decision Stumps as implemented by sklearn. Take a look at the class skeleton first so that you understand the underlying organization and data structures that we'll be using.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T17:43:35.174102Z",
     "start_time": "2018-04-30T17:43:34.945064Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, n_learners=20, base=DecisionTreeClassifier(max_depth=1), random_state=1234):\n",
    "        \"\"\"\n",
    "        Create a new adaboost classifier.\n",
    "        \n",
    "        Args:\n",
    "            N (int, optional): Number of weak learners in classifier.\n",
    "            base (BaseEstimator, optional): Your general weak learner \n",
    "            random_state (int, optional): set random generator.  needed for unit testing. \n",
    "\n",
    "        Attributes:\n",
    "            base (estimator): Your general weak learner \n",
    "            n_learners (int): Number of weak learners in classifier.\n",
    "            alpha (ndarray): Coefficients on weak learners. \n",
    "            learners (list): List of weak learner instances. \n",
    "        \"\"\"\n",
    "        \n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        self.n_learners = n_learners \n",
    "        self.base = base\n",
    "        self.alpha = np.zeros(self.n_learners)\n",
    "        self.learners = []\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train AdaBoost classifier on data. Sets alphas and learners. \n",
    "        \n",
    "        Args:\n",
    "            X_train (ndarray): [n_samples x n_features] ndarray of training data   \n",
    "            y_train (ndarray): [n_samples] ndarray of data \n",
    "        \"\"\"\n",
    "\n",
    "        # =================================================================\n",
    "        # TODO \n",
    "\n",
    "        # Note: You can create and train a new instantiation \n",
    "        # of your sklearn decision tree as follows \n",
    "\n",
    "        # w = np.ones(len(y_train))\n",
    "        # h = clone(self.base)\n",
    "        # h.fit(X_train, y_train, sample_weight=w)\n",
    "        # =================================================================\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Adaboost prediction for new data X.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            \n",
    "        Returns: \n",
    "            yhat (ndarray): [n_samples] ndarray of predicted labels {-1,1}\n",
    "        \"\"\"\n",
    "\n",
    "        # =================================================================\n",
    "        # TODO \n",
    "\n",
    "        return np.zeros(X.shape[0])\n",
    "        # =================================================================\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes prediction accuracy of classifier.  \n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            y (ndarray): [n_samples] ndarray of true labels  \n",
    "            \n",
    "        Returns: \n",
    "            Prediction accuracy (between 0.0 and 1.0).\n",
    "        \"\"\"\n",
    "        pred = self.predict(X)\n",
    "        return np.sum(y == pred)/len(y)\n",
    "    \n",
    "    def staged_score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes the ensemble score after each iteration of boosting \n",
    "        for monitoring purposes, such as to determine the score on a \n",
    "        test set after each boost.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            y (ndarray): [n_samples] ndarray of true labels  \n",
    "            \n",
    "        Returns: \n",
    "            scores (ndarary): [n_learners] ndarray of scores \n",
    "        \"\"\"\n",
    "\n",
    "        scores = []\n",
    "        \n",
    "        yhat = np.zeros(X.shape[0])\n",
    "        for a, h in zip(self.alpha, self.learners):\n",
    "            yhat += a * h.predict(X)\n",
    "            scores.append(np.sum(np.sign(yhat)==y)/len(y))\n",
    "            \n",
    "        return np.array(scores)\n",
    "    \n",
    "    def staged_margin(self, x, y):\n",
    "        \"\"\"\n",
    "        Computes the staged margin after each iteration of boosting \n",
    "        for a single training example x and true label y\n",
    "        \n",
    "        Args:\n",
    "            x (ndarray): [n_features] ndarray of data \n",
    "            y (integer): an integer {-1,1} representing the true label of x \n",
    "            \n",
    "        Returns: \n",
    "            margins (ndarary): [n_learners] ndarray of margins \n",
    "        \"\"\"\n",
    "        \n",
    "        norm_alphas = self.alpha / np.sum(self.alpha)\n",
    "        \n",
    "        margins = np.zeros(self.n_learners)\n",
    "        for kk, h in enumerate(self.learners):\n",
    "            contrib = (-1.0)**((h.predict([x]) == y)+1) * norm_alphas[kk]\n",
    "            if kk == 0:\n",
    "                margins[kk] = contrib \n",
    "            else:\n",
    "                margins[kk] = margins[kk-1] + contrib\n",
    "       \n",
    "        return margins\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the model we attempt to learn in AdaBoost is given by \n",
    "\n",
    "$$\n",
    "H({\\bf x}) = \\textrm{sign}\\left[\\displaystyle\\sum_{k=1}^K\\alpha_k h_k({\\bf x}) \\right]\n",
    "$$\n",
    "\n",
    "where $h_k({\\bf x})$ is the $k^\\textrm{th}$ weak learner and $\\alpha_k$ is it's associated ensemble coefficient.  \n",
    "\n",
    "**Part A**: Implement the `fit` method to learn the sequence of weak learners $\\left\\{h_k({\\bf x})\\right\\}_{k=1}^K$ and corresponding coefficients $\\left\\{ \\alpha_k\\right\\}_{k=1}^K$. Note that you may use sklearn's implementation of DecisionTreeClassifier as your weak learner which allows you to pass as an optional parameter the weights associated with each training example.  An example of instantiating and training a single learner is given in the comments of the `fit` method.  \n",
    "\n",
    "Recall that the AdaBoost algorithm is as follows: \n",
    "\n",
    "`for k=1 to K:`\n",
    "\n",
    "$~~~~~~~$ `    a) Fit kth weak learner to training data with weights w`\n",
    "\n",
    "$~~~~~~~$ `    b) Computed weighted error errk for the kth weak learner` \n",
    "\n",
    "$~~~~~~~$ `    c) compute vote weight alpha[k] = 0.5 ln ((1-errk)/errk))`\n",
    "\n",
    "$~~~~~~~$ `    d) update training example weights w[i] *= exp[-alpha[k] y[i] h[k](x[i])`\n",
    "\n",
    "$~~~~~~~$ `    e) normalize training weights so they sum to 1`\n",
    "\n",
    "When you think you're done, run your method on the following data, which corresponds to the example presented in [lecture](https://www.cs.colorado.edu/~ketelsen/files/courses/csci4622/slides/lesson37.pdf).  Do your computed values of `alpha` look right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T17:43:36.754064Z",
     "start_time": "2018-04-30T17:43:36.741608Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[6,9.5],[4,8.5],[9,8.75],[8,8.0],[3,7],[1,6.5],[5,6.5],[1.5,2.5],[2,1],[9,2]])\n",
    "y = np.array([1,1,-1,1,-1,1,-1,1,-1,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: After your `fit` method is working properly, implement the `predict` method to make predictions for unseen examples stored in a data matrix ${\\bf X}$.  \n",
    "\n",
    "**Note**: Remember that AdaBoost assumes that your predictions are of the form $y \\in \\{-1, 1\\}$. \n",
    "\n",
    "When you think you're done make predictions on the training data and see if you get the expected 100% accuracy. Then look at the graph of the final AdaBoost decision boundary given in the lecture slides, and pick validation examples to predict on (note that the training data lives on $[0,10] \\times [0,10]$).  Do your results agree with the decision boundary seen in the slides?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T17:43:38.781773Z",
     "start_time": "2018-04-30T17:43:38.775334Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: AdaBoost for Handwritten Digits \n",
    "***\n",
    "\n",
    "Use your AdaBoost code with Sklearn's DecisionTreeClassifier as the base learner to distinguish $3$'s from $8$'s. \n",
    "Run $n=500$ boosting iterations with trees of depths 1, 2, and 3 (go deeper if you like) as the weak learner. For each weak learner, plot the training and validation error per boosting iteration on the same set of axes (note, the `staged_score` function will be helpful for this). Compare and contrast the different weak learners. Which works the best? Do you see signs of overfitting? Do any of classifiers achieve nearly 100% accuracy on the training data? What happens to the accuracy on the validation data on further iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T17:43:55.504383Z",
     "start_time": "2018-04-30T17:43:41.585314Z"
    }
   },
   "outputs": [],
   "source": [
    "mab = # TODO \n",
    "\n",
    "train_scores = # TODO \n",
    "valid_scores = # TODO \n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,4))\n",
    "ax.plot(1-train_scores, label=\"train\")\n",
    "ax.plot(1-valid_scores, label=\"valid\")\n",
    "ax.set_xlabel(\"boosting iteration\", fontsize=16)\n",
    "ax.set_ylabel(\"misclassification error\", fontsize=16)\n",
    "ax.grid(alpha=0.25)\n",
    "ax.legend(loc=\"upper right\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
